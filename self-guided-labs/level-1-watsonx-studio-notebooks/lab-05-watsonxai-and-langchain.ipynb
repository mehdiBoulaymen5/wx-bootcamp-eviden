{"cells": [{"metadata": {}, "id": "3c203bbd", "cell_type": "markdown", "source": "# Lab 5: Using LangChain with IBM WatsonX\n\n## 1. Intro to LangChain\n\n[LangChain](https://docs.langchain.com/docs/) is an open-source development framework designed to simplify the creation of applications using large language models (LLMs).\n\nThe core idea of the library is that we can \"chain\" together different components to create more advanced use cases around LLMs. Here are the main components for the LangChain\n\n- Model: interact with various LLMs\n- Prompts: text that is sent to the LLMs\n- Chains: allow to combine different LLM calls and actions automatically\n- Embeddings and Vector Stores: break large data into chunks and store those to be queried when relevant\n- Agents: enbale the LLMs to dynamically decide which tools to use in order to best respond to a given query\n\nIn short, **Langchain is a framework that can orchestrate a series of prompts to achieve a desired outcomes.**\n"}, {"metadata": {}, "id": "917c30de", "cell_type": "markdown", "source": "## 2. How to connect LangChain to WatsonX.ai"}, {"metadata": {}, "id": "a924effc", "cell_type": "code", "source": "!pip install \"ibm-generative-ai[langchain]\"", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "51ee44f4", "cell_type": "code", "source": "!pip install pypdf", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d2ea7a1c", "cell_type": "code", "source": "!pip install sentence_transformers", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "149266ad", "cell_type": "code", "source": "!pip install chromadb", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "4adcdb35", "cell_type": "code", "source": "import os, getpass\nimport requests\n# from dotenv import load_dotenv\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom typing import Any, List, Mapping, Optional, Union, Dict\nfrom pydantic import BaseModel, Extra\ntry:\n    from langchain import PromptTemplate\n    from langchain.chains import LLMChain, SimpleSequentialChain\n    from langchain.document_loaders import PyPDFLoader\n    from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb  # does not work in Cloud project\n    from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace embedding models\n    from langchain.text_splitter import CharacterTextSplitter #text splitter\n    from langchain.llms.base import LLM\n    from langchain.llms.utils import enforce_stop_tokens\nexcept ImportError:\n    raise ImportError(\"Could not import langchain: Please install ibm-generative-ai[langchain] extension.\")\n\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams", "execution_count": 38, "outputs": []}, {"metadata": {}, "id": "96339420", "cell_type": "code", "source": "#config Watsonx.ai environment\nendpoint_url = input(\"Please enter your WML endpoint url (hit enter): \")", "execution_count": 17, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Please enter your WML endpoint url (hit enter): https://us-south.ml.cloud.ibm.com\n"}]}, {"metadata": {}, "id": "cc0061fd", "cell_type": "code", "source": "api_key = getpass.getpass(\"Please enter your IBM Cloud api key (hit enter): \")", "execution_count": 18, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Please enter your IBM Cloud api key (hit enter): \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {}, "id": "53bdda0b", "cell_type": "code", "source": "access_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n).get_token()", "execution_count": 19, "outputs": []}, {"metadata": {}, "id": "bb8f483e", "cell_type": "code", "source": "try:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": 20, "outputs": []}, {"metadata": {}, "id": "e1a5a1e3", "cell_type": "code", "source": "# Helper Function\nclass Prompt:\n    def __init__(self, access_token, project_id):\n        self.access_token = access_token\n        self.project_id = project_id\n\n    def generate(self, input, model_id, parameters):\n        wml_url = f\"{endpoint_url}/ml/v1-beta/generation/text?version=2023-05-28\"\n        Headers = {\n            \"Authorization\": \"Bearer \" + self.access_token,\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\"\n        }\n        data = {\n            \"model_id\": model_id,\n            \"input\": input,\n            \"parameters\": parameters,\n            \"project_id\": self.project_id\n        }\n        response = requests.post(wml_url, json=data, headers=Headers)\n        if response.status_code == 200:\n            return response.json()[\"results\"][0]\n        else:\n            return response.text", "execution_count": 21, "outputs": []}, {"metadata": {}, "id": "56ea58d4", "cell_type": "code", "source": "##initializing WatsonX model\n\n# Parameters\nmodel_params = {\n         \"decoding_method\": \"sample\",\n         \"min_new_tokens\": 1,\n         \"max_new_tokens\": 100,\n         \"random_seed\": 42,\n         \"temperature\": 0.5,\n         \"top_k\": 50,\n         \"top_p\": 1\n}\n\n# Model ID\nmodel_id=\"google/flan-ul2\"", "execution_count": 22, "outputs": []}, {"metadata": {}, "id": "a61f849d", "cell_type": "code", "source": "prompt = Prompt(access_token, project_id)", "execution_count": 23, "outputs": []}, {"metadata": {}, "id": "f35b2832", "cell_type": "code", "source": "# test that everything was set up correctly\nresponse = prompt.generate(\"The sky is ...\", model_id, model_params)", "execution_count": 24, "outputs": []}, {"metadata": {}, "id": "06c61bd1", "cell_type": "code", "source": "response['generated_text']", "execution_count": 25, "outputs": [{"output_type": "execute_result", "execution_count": 25, "data": {"text/plain": "'blue'"}, "metadata": {}}]}, {"metadata": {}, "id": "83e37376-3f85-4751-9aba-9550af6cbf37", "cell_type": "markdown", "source": "In order to use WatsonX-based LLMs with Langchain, the LLM object must be of class `BaseLanguageModel` (see [Langchain docs](https://api.python.langchain.com/en/latest/schema/langchain.schema.language_model.BaseLanguageModel.html)). We'll use the custom class below to accomplish this."}, {"metadata": {}, "id": "805b7808", "cell_type": "code", "source": "# Wrap the WatsonX Model in a langchain.llms.base.LLM subclass to allow LangChain to interact with the model\n\nclass LangChainInterface(LLM, BaseModel):\n    credentials: Optional[Dict] = None\n    model: Optional[str] = None\n    params: Optional[Dict] = None\n    project_id : Optional[str]=None\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n        extra = Extra.forbid\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        _params = self.params or {}\n        return {\n            **{\"model\": self.model},\n            **{\"params\": _params},\n        }\n    \n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return \"IBM WATSONX\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        \"\"\"Call the WatsonX model\"\"\"\n        params = self.params or {}\n        model = Model(model_id=self.model, params=params, credentials=self.credentials, project_id=self.project_id)\n        text = model.generate_text(prompt)\n        if stop is not None:\n            text = enforce_stop_tokens(text, stop)\n        return text", "execution_count": 26, "outputs": []}, {"metadata": {}, "id": "0b59dd99", "cell_type": "code", "source": "creds = {\n        \"url\": endpoint_url,\n        \"apikey\": api_key\n    }", "execution_count": 27, "outputs": []}, {"metadata": {}, "id": "476ecd0d", "cell_type": "code", "source": "llm_model = LangChainInterface(model='google/flan-ul2', credentials=creds, params=model_params, project_id=project_id)", "execution_count": 28, "outputs": []}, {"metadata": {}, "id": "1b25c008", "cell_type": "code", "source": "##predict with the model\ntext = \"What is the capital of South Korea?\"\nllm_model(text)", "execution_count": 29, "outputs": [{"output_type": "execute_result", "execution_count": 29, "data": {"text/plain": "'seoul'"}, "metadata": {}}]}, {"metadata": {}, "id": "a52e2e8f", "cell_type": "markdown", "source": "## 3. Prompt Templates & Chains\n\nIn the previous example, the user input is sent directly to the LLM. However, when using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios\n\n- Accepting user input and contruct a prompt\n- Generating mutiple prompts from an collection of data points in a dataset "}, {"metadata": {}, "id": "310c2bbc", "cell_type": "code", "source": "# Define the prompt templates\nprompt = PromptTemplate(\n  input_variables=[\"country\"],\n  template= \"where is the capital of {country}?\",\n)\n\n# Chaining \nchain = LLMChain(llm=llm_model, prompt=prompt)\n\n# Getting predictions\ncountries = [\"USA\", \"England\", \"Japan\", \"Saudi Arabia\"]\nfor country in countries:\n    response = chain.run(country)\n    print(prompt.format(country=country) + \" = \" + response)", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "where is the capital of USA? = washington dc\nwhere is the capital of England? = london\nwhere is the capital of Japan? = tokyo\nwhere is the capital of Saudi Arabia? = jeddah\n", "name": "stdout"}]}, {"metadata": {}, "id": "918a9df3", "cell_type": "markdown", "source": "## 4. Simple sequential chains\nThe utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n\nLangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."}, {"metadata": {}, "id": "ffda7c24", "cell_type": "code", "source": "## Create two sequential prompts \npt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\npt2 = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Answer the following question: {question}\",\n)", "execution_count": 31, "outputs": []}, {"metadata": {}, "id": "23e4e1ee", "cell_type": "code", "source": "flan = LangChainInterface(model='google/flan-ul2', credentials=creds, params=model_params, project_id=project_id)\nmodel = LangChainInterface(model='google/flan-ul2', credentials=creds, project_id=project_id)", "execution_count": 32, "outputs": []}, {"metadata": {}, "id": "35de1e4d", "cell_type": "code", "source": "prompt_to_flan = LLMChain(llm=flan, prompt=pt1)\nflan_to_model = LLMChain(llm=model, prompt=pt2)\nqa = SimpleSequentialChain(chains=[prompt_to_flan, flan_to_model], verbose=True)", "execution_count": 33, "outputs": []}, {"metadata": {}, "id": "34586549", "cell_type": "code", "source": "qa.run(\"the meaning of life\")", "execution_count": 34, "outputs": [{"output_type": "stream", "text": "\n\n\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n\u001b[36;1m\u001b[1;3mWhat does the word \"eudaimonia\" mean?\u001b[0m\n\u001b[33;1m\u001b[1;3mgood life\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 34, "data": {"text/plain": "'good life'"}, "metadata": {}}]}, {"metadata": {}, "id": "ed7c152d", "cell_type": "markdown", "source": "## 5. Easy Loading of Documents Using Lang Chain\nLangChain makes it easy to extract passages from documents so that you can answering questions based on your document's content."}, {"metadata": {}, "id": "21e350aa", "cell_type": "code", "source": "# !pip install -U sqlalchemy", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "b1743ccd", "cell_type": "code", "source": "# Locally we load the PDF file\n# pdf='what is generative ai.pdf'\n# loaders = [PyPDFLoader(pdf)]\n\n# To make our lives a bit easier with this Cloud version notebook, we will load a PDF from a URL instead\nloaders = [PyPDFLoader('https://www.mckinsey.com/~/media/mckinsey/featured%20insights/mckinsey%20explainers/what%20is%20generative%20ai/what%20is%20generative%20ai.pdf')]", "execution_count": 39, "outputs": []}, {"metadata": {}, "id": "9348f8c4", "cell_type": "code", "source": "index = VectorstoreIndexCreator(\n    embedding=HuggingFaceEmbeddings(),\n    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "43a890f9", "cell_type": "code", "source": "###initializing watsonx flan_ul2 model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.MIN_NEW_TOKENS: 50,\n    GenParams.MAX_NEW_TOKENS: 300,\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_K: 100,\n    GenParams.TOP_P:1\n}\n\nmodel = LangChainInterface(model='google/flan-ul2', credentials=creds, params=params, project_id=project_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "7d1a50d6", "cell_type": "code", "source": "from langchain.chains import RetrievalQA\nchain = RetrievalQA.from_chain_type(llm=model, \n                                    chain_type=\"stuff\", \n                                    retriever=index.vectorstore.as_retriever(), \n                                    input_key=\"question\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "2b6efed8", "cell_type": "code", "source": "##answering based on the documents \nchain.run(\"what is generative ai?\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "5d21ffff", "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}