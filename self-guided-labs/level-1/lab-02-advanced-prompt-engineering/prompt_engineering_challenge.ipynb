{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Prompt Lab Challenge Exercises Notebook"}, {"metadata": {}, "cell_type": "markdown", "source": "Welcome to the second prompt lab in the bootcamp series, you should have completed lab 1 and the exercises follow on from those. If you completed all the exercises in Lab 1 you should find most of the exercises here straightforward\n\nThis notebook is a template with all the exercises and indications of what the output should look like if you do a good job with the prompts.\n\nBefore you start you should have a Python environment with the necessary libraries installed as indicated in the intro lab, you will also need a .env file with: \n- your IBM Cloud API key\n- the IBM Cloud regional URL (eg, https://us-south.ml.cloud.ibm.com)\n- the project ID associated with your WatsonX project (required by the WML Python SDK)\n\nIt should take you about 30-45 min to walk through the exercises self paced\n\nGood luck and make sure you compare your answers with the model solutions\n"}, {"metadata": {}, "cell_type": "code", "source": "import os\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\napi_key = \nibm_cloud_url = \"https://eu-de.ml.cloud.ibm.com\"\nproject_id = \"00e43b61-b7a6-4ed9-aabf-3650f23d4b91\"", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "2. Load credentials for Watsonx.ai (note refer to lab explaining how to do this if necessary)\n    - you should have a .env file with your IBM Cloud API key, eg API_KEY=xxx\n    - you should have a .env with the IBM Cloud regional url, eg IBM_CLOUD_URL=https://eu-de.ml.cloud.ibm.com\n    - you should have a .env with the associated WatsonX project ID, eg PROJECT_ID=xxx"}, {"metadata": {}, "cell_type": "code", "source": "#config Watsonx.ai environment\n\nif api_key is None or ibm_cloud_url is None or project_id is None:\n    print(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\nelse:\n    creds = {\n        \"url\": ibm_cloud_url,\n        \"apikey\": api_key \n    }", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Helper function for text generation with the [WML Python SDK](https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html) for foundation models."}, {"metadata": {}, "cell_type": "code", "source": "def send_to_watsonxai(prompts,\n                    model_name=\"google/flan-ul2\",\n                    decoding_method=\"greedy\",\n                    max_new_tokens=100,\n                    min_new_tokens=30,\n                    temperature=1.0,\n                    repetition_penalty=2.0\n                    ):\n    '''\n   helper function for sending prompts and params to Watsonx.ai\n    \n    Args:  \n        prompts:list list of text prompts\n        decoding:str Watsonx.ai parameter \"sample\" or \"greedy\"\n        max_new_tok:int Watsonx.ai parameter for max new tokens/response returned\n        temperature:float Watsonx.ai parameter for temperature (range 0>2)\n        repetition_penalty:float Watsonx.ai parameter for repetition penalty (range 1.0 to 2.0)\n\n    Returns: None\n        prints response\n    '''\n\n    assert not any(map(lambda prompt: len(prompt) < 1, prompts)), \"make sure none of the prompts in the inputs prompts are empty\"\n\n    # Instantiate parameters for text generation\n    model_params = {\n        GenParams.DECODING_METHOD: decoding_method,\n        GenParams.MIN_NEW_TOKENS: min_new_tokens,\n        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n        GenParams.RANDOM_SEED: 42,\n        GenParams.TEMPERATURE: temperature,\n        GenParams.REPETITION_PENALTY: repetition_penalty,\n    }\n\n\n    # Instantiate a model proxy object to send your requests\n    model = Model(\n        model_id=model_name,\n        params=model_params,\n        credentials=creds,\n        project_id=project_id)\n\n\n    for prompt in prompts:\n        print(model.generate_text(prompt))\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q0) Basic prompt example\n\nThe following is a basic example of how you can structure your prompt in Python, useful for people who aren't familiar with the language. You can just run the following cell and see how it goes."}, {"metadata": {}, "cell_type": "code", "source": "#Q0 - Example prompt\nprompt = \"\"\"Capture entities in the given input.\n\nInput:\nIBM is a competitor of Google in the US said Rami Swarthmore.\n\nEntities:\nIBM: company, Google: company, US: country, Rami Swarthmore: person\n\nInput:\nMicrosoft has seen declining market share in the EU, Eric Schmidt said.\n\nEntities:\n\"\"\"\nresponse = send_to_watsonxai(prompts=[prompt]) \n\n# Expected outpput: Eric Schmidt: person, Microsoft: company, EU: region", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Eric Schmidt: person, Microsoft: company, EU: region, declining market share: entity, declining market share: unit of measurement, Microsoft: product\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Note that Q1 is challenging - consider doing it last in the lab\n#### Q1) Basic inference: A patients a1c level determines their diabetes status, the rules are as follows:\n\n - less than 5.7 no diabetes\n - between 5.7 and 6.5 pre-diabetes\n - greater than 6.5 diabetic.\n\nWrite a prompt to return just the diabetes status from the following 3 test cases:\n\n1)\tThe patients a1c is 5.5 which is good considering his other risk factors.\n2)\tFrom the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\n3)\tShe mentioned her A1c is 8 according to her blood work about 3 years ago.\n\nBonus 1: How could you improve the inference given the other information in the sentences?\n\nBonus 2: how would you approach extracting the diabetes status based on patient notes without A1C values and what would you need to watch out for? (hint: maybe they are talking about family history of disease or other complications)\n"}, {"metadata": {}, "cell_type": "code", "source": "#Q1 ENTER YOUR MODEL PARAMS HERE - MAKE SURE IT WORKS WITH ALL 3 EXAMPLES ABOVE\nprompt = #complete your prompt here\nresponse = send_to_watsonxai(prompts=[prompt]) ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Product Review for Questions  2-6\nreview = \"\"\"Needed a nice lamp for my bedroom, and this one had \\\nadditional storage and not too high of a price point. \\\nGot it fast.  The string to our lamp broke during the \\\ntransit and the company happily sent over a new one. \\\nCame within a few days as well. It was easy to put \\\ntogether.  I had a missing part, so I contacted their \\\nsupport and they very quickly got me the missing piece! \\\nLumina seems to me to be a great company that cares \\\nabout their customers and products!!\"\"\"", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q2) write a prompt to return the sentiment of the review\nTarget sentiment = positive"}, {"metadata": {}, "cell_type": "code", "source": "#Q2 Code - enter prompt and parameters in this cell\nprompt = #Complete your prompt here \nresponse = send_to_watsonxai(prompts=[prompt])", "execution_count": 12, "outputs": [{"name": "stdout", "output_type": "stream", "text": ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q3) extract the emotions the reviewer expressed, return answer as a comma separated list\nTarget emotions = satisfied, happy, cared for, great company, product!"}, {"metadata": {}, "cell_type": "code", "source": "prompt = \" \"\nresponse = send_to_watsonxai(prompts=[prompt])", "execution_count": 13, "outputs": [{"name": "stdout", "output_type": "stream", "text": ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q4) Is the reviewer expressing anger, answer \u201cyes\u201d or \u201cno\u201d \u2013 test with your own example including anger to ensure it works in both cases.\nTarget answer = no"}, {"metadata": {}, "cell_type": "code", "source": "prompt = #Complete your prompt here\nresponse = send_to_watsonxai(prompts=[prompt])\nprint(response)", "execution_count": 14, "outputs": [{"name": "stdout", "output_type": "stream", "text": ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\nNone\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q5) Extract the item purchased and the company name, return as JSON format\nTarget answer = Item[lamp], Brand[Lumina]"}, {"metadata": {}, "cell_type": "code", "source": "prompt = f\"\"\"\nIdentify the following items from the review text: \n- Item purchased by reviewer\n- Company that made the item\n\nThe review is delimited with triple backticks. \\\nFormat your response as a JSON object with \\\n\"Item\" and \"Brand\" as the keys. \nIf the information isn't present, use \"unknown\" \\\nas the value.\nMake your response as short as possible.\n  \nReview text: '''{review}'''\n\"\"\"\n\nprompt = #complete your prompt here\nresponse = send_to_watsonxai(prompts=[prompt])\nprint(response)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q6) Can you combine 3-6 in a single prompt and return JSON with: Sentiment (negative or positive), Anger (yes/no), Product, Company\nTarget answer = Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]"}, {"metadata": {}, "cell_type": "code", "source": "prompt = #Complete your prompt here \nresponse = send_to_watsonxai(prompts=[prompt])\nprint(response)", "execution_count": null, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q7) summarize the following product review\nExample summary = My daughter loves it!  It's soft and  super cute, and its face has a friendly look. It's  a bit small for what I paid though."}, {"metadata": {}, "cell_type": "code", "source": "review = \"\"\"Got this panda plush toy for my daughter's birthday, \\\nwho loves it and takes it everywhere. It's soft and \\ \nsuper cute, and its face has a friendly look. It's \\ \na bit small for what I paid though. I think there \\ \nmight be other options that are bigger for the \\ \nsame price. It arrived a day earlier than expected, \\ \nso I got to play with it myself before I gave it to her.\"\"\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "prompt = #Complete your prompt here\nresponse = send_to_watsonxai(prompts=[prompt])\nprint(response) ", "execution_count": null, "outputs": [{"name": "stdout", "output_type": "stream", "text": "My daughter loves it!  It's soft and  super cute, and its face has a friendly look. It's  a bit small for what I paid though.\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q8) Summarize the same product review from the perspective of the shipping department\nExample summary = It arrived a day earlier than expected, so I got to play with it myself before I gave it  to her. "}, {"metadata": {}, "cell_type": "code", "source": "#concise wrt feedback shipping\nprompt = #Complete your prompt here\nresponse = send_to_watsonxai(prompts=[prompt])\nprint(response)", "execution_count": null, "outputs": [{"name": "stdout", "output_type": "stream", "text": "It arrived a day earlier than expected,  so I got to play with it myself before I gave it  to her. \n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q9) Summarize the review from the perspective of pricing and value\nExample summary = It's a bit small for what I paid though. I think there might be other options that are bigger for the same price"}, {"metadata": {}, "cell_type": "code", "source": "#feedback pricing works - concise\nprompt = #Complete your prompt here \nresponse = send_to_watsonxai(prompts=[prompt])\nprint(response)", "execution_count": null, "outputs": [{"name": "stdout", "output_type": "stream", "text": "It's a bit small for what I paid though. I think there  might be other options that are bigger for the  same price\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Q10)\tPII removal. Given the following email, write a prompt to remove the PII (eg names, emails etc) (Hint: you may need to use 1-2 shot technique)"}, {"metadata": {}, "cell_type": "code", "source": "email = \"\"\"\nHi John,\\\n\nI'm writing to you because I noticed you recently purchased a new car. I'm a salesperson\\\nat a local dealership (Cheap Dealz), and I wanted to let you know that we have a great deal on a new\\\ncar. If you're interested, please let me know.\\\n\nThanks,\\\n\nJimmy Smith\\\n\nPhone: 410-805-2345\\\nEmail: jimmysmith@cheapdealz.com\\\n\"\"\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Hint - use prompt template or manually construct the prompt with f strings (look up in documentation if unsure)"}, {"metadata": {}, "cell_type": "code", "source": "prompt = #Complete your prompt here \nresponse = send_to_watsonxai(prompts=[prompt])\nprint(response)", "execution_count": 16, "outputs": [{"name": "stdout", "output_type": "stream", "text": ". [1] The first of these is the so-called \"separate existence\" doctrine, which holds that an individual'[nonexistence or inability to perform certain functions) cannot be held legally responsible for his/her failure (or lack thereof).\nNone\n"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}