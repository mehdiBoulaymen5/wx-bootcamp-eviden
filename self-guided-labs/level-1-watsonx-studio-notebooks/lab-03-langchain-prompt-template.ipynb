{"cells": [{"metadata": {}, "id": "abb283bb", "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n# Insert the project token by clicking the three vertical dots in the top bar and then \"Insert project token\". This will allow you to read files from this Watson Studio project environment\n# Make sure not to share it ", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# INSERT PROJECT TOKEN HERE\n\n", "execution_count": 28, "outputs": []}, {"metadata": {}, "id": "b1788701", "cell_type": "markdown", "source": "# Lab 3: Intro to Building Prompt Templates with LangChain\n\n### If you have troubles with your local installation you can use this notebook instead. \n\nPlease note that compared to the local version the set-up and the way the model is accessed and credentiasl are read are slightly different. "}, {"metadata": {}, "id": "21c1bb64", "cell_type": "markdown", "source": "Welcome to the Lab 3. \n\nIn the previous lab, we explored the challenges of prompt engineering; learning how to tweak our wording, choose different models, plus optimize model parameters. Minor changes can significantly enhance the results generated by language models.\n\nIn this lab, we will apply our new knowledge to a real-world use case as we continue learning about best practices related to prompt coding. Using the [Watson Machine learning Python SDK](https://ibm.github.io/watson-machine-learning-sdk/) to programmatically interact with watsonx.ai, we will use prompt templating techniques provided by the [LangChain Python library](https://python.langchain.com/) to streamline our interactions with the language model and maximize its potential.\n\nThe concept of Prompt Templates provided by LangChain allows you to construct prompt templates that can be easily filled with specific information to generate a wide range of outputs that you can provide to watsonx.ai. You can even leverage prompt templates specific to few-shot prompting, as you will see below."}, {"metadata": {}, "id": "6110e08a", "cell_type": "markdown", "source": "## Recreating Prompt Builder Prompts Using LangChain Prompt Patterns"}, {"metadata": {}, "id": "d991e3a4", "cell_type": "markdown", "source": "### Scenario: Personalized Recommendation for XYZ Retail Company <a id=\"step3\"></a>"}, {"metadata": {}, "id": "4c7f1fb7", "cell_type": "markdown", "source": "XYZ Retail is a popular online retail store that sells a wide range of products, including electronics, clothing, home goods, and more. They have a large customer base and want to provide a personalized shopping experience to enhance customer satisfaction and boost sales.\n\nTo achieve that goal, XYZ wants to leverage generative AI to create fact sheets about each of their customers. These fact sheets will summarize relevant information such as customer demographics (name, age, location), and purchase history. These fact sheets will help XYZ Retail's sales team build stronger customer relationships, increase customer satisfaction and drive repeat purchases.\n"}, {"metadata": {}, "id": "d91d30ff", "cell_type": "markdown", "source": "You start by performing prompt engineering in Prompt Lab, and you might test base model output with an initial prompt like this:\n\n![title](./images/prompt_without_example.png)"}, {"metadata": {}, "id": "494a7da9", "cell_type": "markdown", "source": "The model's recommendation is not accurate or useful as the customer Michael Jones had bought toys and games not outdoor activewear. Fortunately you learned in the Prompt Engineering lab that Few Shot Learning can help you obtain better results. \n\nWhat happens when we provide a few examples using Prompt Builder to guide the LLM into generating more meaningful recommendations. \n\n![title](./images/prompt_with_example.png)\n"}, {"metadata": {}, "id": "20fad598", "cell_type": "markdown", "source": "Great, the product recommendation for Michael Jones is much better.  However how do you productionize your few shot prompting to generate recommendations for all of XYZ Retail customers? Copy and pasting each customer's info into Prompt Builder would take too long.  \n\nYou'll need a programmatic solution.  Maybe you could even generate a large set of examples then use that for Tuning a model in watsonx.ai.  But we're getting ahead of ourselves as you'll learn about building a Prompt Tuning dataset in a later lab."}, {"metadata": {}, "id": "16a4d9fa", "cell_type": "markdown", "source": "## 1. Load the required libraries  <a id=\"step1\"></a>"}, {"metadata": {}, "id": "57cf7118", "cell_type": "code", "source": "# due to some Python version issue together with Langchain with the Python version available on Cloud we cannot use standard LangChain installation and must go through generative ai extension instead at the time being\n# !pip install langchain\n# !pip install typing-extensions\n!pip install \"ibm-generative-ai[langchain]\"", "execution_count": null, "outputs": []}, {"metadata": {"tags": []}, "id": "212e985c", "cell_type": "code", "source": "import os\n\nimport pandas as pd\nfrom langchain import PromptTemplate, FewShotPromptTemplate\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nimport os, getpass\nimport requests\nfrom ibm_cloud_sdk_core import IAMTokenManager", "execution_count": 5, "outputs": []}, {"metadata": {}, "id": "80426145", "cell_type": "markdown", "source": "# Setup credentials"}, {"metadata": {}, "id": "fa33171f", "cell_type": "markdown", "source": "In the next steps, you need the credentials mentioned above, especially your IBM Cloud API key and IBM Cloud regional URL (eg, https://us-south.ml.cloud.ibm.com)"}, {"metadata": {}, "id": "cb9b3a9e", "cell_type": "markdown", "source": "# Enter WML endpoint URL"}, {"metadata": {}, "id": "7bab3a1b", "cell_type": "code", "source": "endpoint_url = input(\"Please enter your WML endpoint url (hit enter): \")", "execution_count": 6, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Please enter your WML endpoint url (hit enter): https://us-south.ml.cloud.ibm.com\n"}]}, {"metadata": {}, "id": "4a325bdd", "cell_type": "markdown", "source": "# Enter API Key"}, {"metadata": {}, "id": "71e0b3a2", "cell_type": "code", "source": "access_token = IAMTokenManager(\n    apikey = getpass.getpass(\"Please enter your IBM Cloud api key (hit enter): \"),\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n).get_token()", "execution_count": 7, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Please enter your IBM Cloud api key (hit enter): \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {}, "id": "6597f803", "cell_type": "markdown", "source": "# Enter Project ID\n\nSince we are working in a Cloud project, you should need to add anything here as it will be read from the notebook environment"}, {"metadata": {}, "id": "bee1d8de", "cell_type": "code", "source": "try:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": 8, "outputs": []}, {"metadata": {}, "id": "aa778df9", "cell_type": "markdown", "source": "# Helper function to set up model endpoint"}, {"metadata": {}, "id": "157aad16", "cell_type": "code", "source": "# Helper Function - slightly different than local helper function\nclass Prompt:\n    def __init__(self, access_token, project_id):\n        self.access_token = access_token\n        self.project_id = project_id\n\n    def generate(self, input, model_id, parameters):\n        wml_url = f\"{endpoint_url}/ml/v1-beta/generation/text?version=2023-05-28\"\n        Headers = {\n            \"Authorization\": \"Bearer \" + self.access_token,\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\"\n        }\n        data = {\n            \"model_id\": model_id,\n            \"input\": input,\n            \"parameters\": parameters,\n            \"project_id\": self.project_id\n        }\n        response = requests.post(wml_url, json=data, headers=Headers)\n        if response.status_code == 200:\n            return response.json()[\"results\"][0]\n        else:\n            return response.text", "execution_count": 9, "outputs": []}, {"metadata": {}, "id": "b9f48ad8", "cell_type": "markdown", "source": "## 2. Create a Factsheet for each customer using Prompt Patterns  <a id=\"step2\"></a>"}, {"metadata": {}, "id": "dd643142", "cell_type": "markdown", "source": "### **2.1 What is a Prompt Template?**\n\nThe [PromptTemplate class](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) in the [LangChain Python library](https://python.langchain.com/docs/get_started/introduction) provides a flexible approach to creating prompts from structured templates.  We will use the PrompTemplate class to simplify creation of our few shot prompts for XYZ Retail.\n\nXYZ Retail has provided you their customer's data in .csv format. To generate prompts for each customer, you will need to transform the prompt that you engineered in Prompt Builder into a more useful programmatic format. Using the PromptTemplate class, you can easily substitute customer data to generate one or multiple prompts.\n\nThe PromptTemplate class defines a schema where variables to replace are placed inside curly braces \"{}\". In Python parlance, it's simply using \"f-strings\" under the hood. These curly braces serve as a placeholder for the actual data that will be substituted into the template.\n\nLet's see how this works in practice."}, {"metadata": {}, "id": "21bcfa88", "cell_type": "markdown", "source": "### **2.2 Creating a simple prompt from a template**\n\nA prompt template can be created using the PromptTemplate class from a string or .txt file. There are [additional PromptTemplate examples](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html#langchain.prompts.prompt.PromptTemplate) provided in the LangChain documentation."}, {"metadata": {}, "id": "1cfc5148", "cell_type": "markdown", "source": "#### 2.2.1 Prompt Template From String"}, {"metadata": {}, "id": "cbd465f4-448c-46e8-b829-79693881df23", "cell_type": "code", "source": "# template is a string with variable names in curly brackets\npattern = \"input: {name} {family_name} is {age} and lives in {location}. They bought {purchase_history}\"\n\n# generate template\nprompt_template = PromptTemplate.from_template(pattern)\nprompt_template.template", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "'input: {name} {family_name} is {age} and lives in {location}. They bought {purchase_history}'"}, "metadata": {}}]}, {"metadata": {}, "id": "90cdc428-2bc7-4062-957e-13f75dabafdd", "cell_type": "code", "source": "# now let's provide some values and generate our prompt\n# notice how the variables coincide with those we specified in curly brackets\nprompt = prompt_template.format(name=\"Jane\", \n                                family_name=\"Doe\",\n                                age=43,\n                                location=\"San Francisco, CA\",\n                                purchase_history = \"groceries, household goods and travel supplies\")\n\nprompt", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "'input: Jane Doe is 43 and lives in San Francisco, CA. They bought groceries, household goods and travel supplies'"}, "metadata": {}}]}, {"metadata": {}, "id": "fe6a6cdb", "cell_type": "markdown", "source": "#### 2.2.2 Prompt Template From File\nPrompt patterns can also be stored as a txt file:"}, {"metadata": {}, "id": "e542a0d3", "cell_type": "code", "source": "# Get the url\n_path_to_file = project.get_file(\"customer_factsheet_lang.txt\").read().decode()", "execution_count": 13, "outputs": []}, {"metadata": {"tags": []}, "id": "72f1cf1f", "cell_type": "code", "source": "# We create a template from a file:\n# this time we provide the variable names in a list\n# due to watson studio file reading options we need to change this a bit compared to the local file and not use the PromptTemplate.from_file\n\n# LOCAL\n# example_prompt = PromptTemplate.from_file(_path_to_file,\n#                                 input_variables=[\"name\", \"family_name\", \"age\",\"city\", \"state\", \n#                                                  \"purchase_history\", \"recommendation_1\", \"recommendation_2\"])\n\n# CLOUD\nexample_prompt = PromptTemplate.from_template(_path_to_file)\n                                # input_variables=[\"name\", \"family_name\", \"age\",\"city\", \"state\", \n                                #                  \"purchase_history\", \"recommendation_1\", \"recommendation_2\"])\n\n\nprint(example_prompt.template)", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "input: \"{name} {family_name} is {age} years old and lives in {city}, {state}. Their purchase history includes {purchase_history}.\"\noutput: \"Recommendations:\\n Item 1: {recommendation_1}\\nItem 2: {recommendation_2}\"\n\n", "name": "stdout"}]}, {"metadata": {}, "id": "2d5647f4", "cell_type": "markdown", "source": "Just like in 2.1, we can populate this teamplate from a dictionary containing the values of the input variables. Looping over three examples:"}, {"metadata": {"tags": []}, "id": "9f8ae595", "cell_type": "code", "source": "# we can iterate through a list to populate the template\n\nexamples = [\n    {\n        \"name\":\"Jane\", \n        \"family_name\":\"Doe\", \n        \"age\":43, \n        \"city\":\"San Francisco\", \n        \"state\":\"CA\",\n        \"purchase_history\":\"groceries, household goods and travel supplies\", \n        \"recommendation_1\":\"Basket of organic fruits\",\n        \"recommendation_2\":\"Lightweight carry-on suitcase\"\n    },{\n        \"name\":\"Siamak\", \n        \"family_name\":\"Baharoo\", \n        \"age\":57, \n        \"city\":\"Chicago\", \n        \"state\":\"IL\",\n        \"purchase_history\":\"Books electronics home_goods\", \n        \"recommendation_1\":\"Kindle Paperwhite - This e-reader is perfect for book lovers who want a lightweight and portable device that can hold thousands of books. It has a glare-free display and a long battery life, so you can read for hours on end without having to worry about running out of power.\",\n        \"recommendation_2\": \"Google Home Mini - This smart speaker is perfect for controlling your home's smart devices with your voice. You can use it to play music, set alarms, get news, and more. It's also a great way to stay connected with friends and family.\"\n    },{\n        \"name\":\"Luis\", \n        \"family_name\":\"Cooli\", \n        \"age\":21, \n        \"city\":\"New York City\", \n        \"state\":\"NY\",\n        \"purchase_history\":\"Clothing shoes cosmetics\", \n        \"recommendation_1\":\"Aritzia Wilfred Free Sweater - This soft and cozy sweater is perfect for a casual day out. It's available in a variety of colors, so you can find the perfect one to match your style.\",\n        \"recommendation_2\":\"Steve Madden Pointed Toe Pumps - These stylish pumps are perfect for a night out on the town. They're comfortable and versatile, so you can wear them with a variety of outfits.\"\n    }\n]\n\nfor example in examples: \n    print(example_prompt.format(**example))", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "input: \"Jane Doe is 43 years old and lives in San Francisco, CA. Their purchase history includes groceries, household goods and travel supplies.\"\noutput: \"Recommendations:\\n Item 1: Basket of organic fruits\\nItem 2: Lightweight carry-on suitcase\"\n\ninput: \"Siamak Baharoo is 57 years old and lives in Chicago, IL. Their purchase history includes Books electronics home_goods.\"\noutput: \"Recommendations:\\n Item 1: Kindle Paperwhite - This e-reader is perfect for book lovers who want a lightweight and portable device that can hold thousands of books. It has a glare-free display and a long battery life, so you can read for hours on end without having to worry about running out of power.\\nItem 2: Google Home Mini - This smart speaker is perfect for controlling your home's smart devices with your voice. You can use it to play music, set alarms, get news, and more. It's also a great way to stay connected with friends and family.\"\n\ninput: \"Luis Cooli is 21 years old and lives in New York City, NY. Their purchase history includes Clothing shoes cosmetics.\"\noutput: \"Recommendations:\\n Item 1: Aritzia Wilfred Free Sweater - This soft and cozy sweater is perfect for a casual day out. It's available in a variety of colors, so you can find the perfect one to match your style.\\nItem 2: Steve Madden Pointed Toe Pumps - These stylish pumps are perfect for a night out on the town. They're comfortable and versatile, so you can wear them with a variety of outfits.\"\n\n", "name": "stdout"}]}, {"metadata": {}, "id": "eb85c4bd", "cell_type": "markdown", "source": "## 3. Create Prompt Examples based on Customers Factsheet <a id=\"step3\"></a>\nThe value of PromptTemplate arises when generating a large number of prompts either as examples for bulk evaluation of an engineered prompt or for creation of a Tuning dataset"}, {"metadata": {}, "id": "f704ef15", "cell_type": "markdown", "source": "### 3.1 Create a few shot prompt\nWe will start by creating a FewShotPromptTemplate object. This class allows to create a prompt made of few repetitions of a PromptTemplate. Details can be found in the [FewShotPromptTemplate class documentation](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples)"}, {"metadata": {"tags": []}, "id": "a4389d2d", "cell_type": "code", "source": "# Next step create a few shot prompt template\n\nfew_shot_examples = examples[:2]\nfew_shot_input = examples[2].copy()\ndel few_shot_input['recommendation_1']\ndel few_shot_input['recommendation_2']\n\ndef make_few_shot_prompt(few_shot_examples, few_shot_input):\n    \"\"\"\n    Generate a few-shot prompt using the FewShotPromptTemplate class.\n\n    Parameters:\n    - few_shot_examples: List of examples to be shown as few-shot examples.\n    - few_shot_input: Input for which the prompt will be generated.\n\n    Returns:\n    - A string representing the formatted few-shot prompt.\n    \"\"\"\n    prompt = FewShotPromptTemplate(\n        examples=few_shot_examples, \n        example_prompt=example_prompt, \n        suffix='input: \"{name} {family_name} is {age} years old and lives in {city}, {state}. Their purchase history includes {purchase_history}.\"\\noutput: ', \n        input_variables=[\"name\", \"family_name\", \"age\", \"city\",\"state\", \"purchase_history\"]\n    )\n    # Return the formatted prompt using the provided input data\n    return prompt.format(**few_shot_input)\n\n\nfew_shot_prompt = make_few_shot_prompt(few_shot_examples, few_shot_input)\nprint(few_shot_prompt)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "input: \"Jane Doe is 43 years old and lives in San Francisco, CA. Their purchase history includes groceries, household goods and travel supplies.\"\noutput: \"Recommendations:\\n Item 1: Basket of organic fruits\\nItem 2: Lightweight carry-on suitcase\"\n\n\ninput: \"Siamak Baharoo is 57 years old and lives in Chicago, IL. Their purchase history includes Books electronics home_goods.\"\noutput: \"Recommendations:\\n Item 1: Kindle Paperwhite - This e-reader is perfect for book lovers who want a lightweight and portable device that can hold thousands of books. It has a glare-free display and a long battery life, so you can read for hours on end without having to worry about running out of power.\\nItem 2: Google Home Mini - This smart speaker is perfect for controlling your home's smart devices with your voice. You can use it to play music, set alarms, get news, and more. It's also a great way to stay connected with friends and family.\"\n\n\ninput: \"Luis Cooli is 21 years old and lives in New York City, NY. Their purchase history includes Clothing shoes cosmetics.\"\noutput: \n", "name": "stdout"}]}, {"metadata": {}, "id": "682e71a4", "cell_type": "markdown", "source": "### 3.2 Bulk Creation of Prompts\nUsing the FewShotPromptTemplate class, we can now create a function that generates a list a few shot prompts populating them iteratively from values directy extracted from a notebook.\n\nWe can choose how many single prompts are include in one few shot prompt. The output of the function is a list of few shot prompts. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Insert 'customer_factsheet.csv' here: "}, {"metadata": {}, "cell_type": "code", "source": "# INSERT CSV FILE\n\n# Insert the file 'customer_factsheet.csv' from the project here.\n# You can do this by pressing the upper-right button with the zeros and ones, then select the csv file and choose \"insert as pandas dataframe\"\n# make sure not to share any credentials from the file \n\n", "execution_count": 26, "outputs": []}, {"metadata": {"tags": []}, "id": "c10d7db8", "cell_type": "code", "source": "# Specify the path to the CSV file containing the data - local only\n# csv_file_path = \"./data/customer_factsheet.csv\"\n\ndef sub_all_from_csv(df_name, n_prompt_examples=2):\n    \"\"\"\n    Generates a list of few-shot prompts using the FewShotPromptTemplate class. \n    The prompts are populated iteratively from values extracted from a CSV file.\n\n    Parameters:\n    - csv_file_path: The path to the CSV file.\n    - n_prompt_examples: The number of examples included in one few-shot prompt.\n\n    Returns:\n    - list_of_prompts: A list of few-shot prompts.\n    \"\"\"\n\n    # df = pd.read_csv(csv_file_path)\n    examples = [example for _, example in df_name.transpose().to_dict().items()]\n\n    i=0\n    list_of_prompts = []\n\n    while i < len(df_name):\n        few_shot_examples = examples[i:i+n_prompt_examples]\n        few_shot_input = examples[i+n_prompt_examples].copy()\n        del few_shot_input['recommendation_1']\n        del few_shot_input['recommendation_2']\n\n        list_of_prompts.append(make_few_shot_prompt(few_shot_examples, few_shot_input))\n        \n        i = i+n_prompt_examples +1\n\n    # Return the list of few-shot prompts\n    return list_of_prompts", "execution_count": 19, "outputs": []}, {"metadata": {"tags": []}, "id": "b6fab6fe", "cell_type": "code", "source": "list_of_prompts = sub_all_from_csv(df)\nprint(list_of_prompts[0])", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "input: \"John Smith is 30 years old and lives in San Francisco, CA. Their purchase history includes Books electronics home_goods.\"\noutput: \"Recommendations:\\n Item 1: Kindle Paperwhite - This e-reader is perfect for book lovers who want a lightweight and portable device that can hold thousands of books. It has a glare-free display and a long battery life, so you can read for hours on end without having to worry about running out of power.\\nItem 2: Google Home Mini - This smart speaker is perfect for controlling your home's smart devices with your voice. You can use it to play music, set alarms, get news, and more. It's also a great way to stay connected with friends and family.\"\n\n\ninput: \"Jane Doe is 25 years old and lives in New York, NY. Their purchase history includes Clothing shoes cosmetics.\"\noutput: \"Recommendations:\\n Item 1: Aritzia Wilfred Free Sweater - This soft and cozy sweater is perfect for a casual day out. It's available in a variety of colors, so you can find the perfect one to match your style.\\nItem 2: Steve Madden Pointed Toe Pumps - These stylish pumps are perfect for a night out on the town. They're comfortable and versatile, so you can wear them with a variety of outfits.\"\n\n\ninput: \"Michael Jones is 40 years old and lives in Seattle, WA. Their purchase history includes Toys games sporting_goods.\"\noutput: \n", "name": "stdout"}]}, {"metadata": {}, "id": "00860a91", "cell_type": "markdown", "source": "### 3.2 Additional Examples\nYou can explore [additional examples using the PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html#langchain.prompts.prompt.PromptTemplate)"}, {"metadata": {}, "id": "ed11a20a", "cell_type": "markdown", "source": "## 4. Prompt evaluation and few shot learning from bulk created prompts <a id=\"step4\"></a>\nIn the prior examples, you created a \"2-shot learning\" prompt.  I.e. there were three inputs but only two complete outputs.  By using a larger dataset this way, you can perform bulk testing of your prompt.\n\nE.g. two of your data samples are used to train before generating the \"output\" of the 3rd. You can now execute these few shot prompts to see how well our engineered prompt works across numerous examples"}, {"metadata": {}, "id": "7e7b5ffb", "cell_type": "markdown", "source": "### 4.1 Import Watsonx.ai access credentials and load model\nMake sure you copied the .env file that you created earlier into the same directory as this notebook"}, {"metadata": {}, "id": "b0ea437e", "cell_type": "code", "source": "# Different than local version\n\n# Parameters\nmodel_params = {\n         \"decoding_method\": \"greedy\",\n         \"min_new_tokens\": 50,\n         \"max_new_tokens\": 100\n}\n\n# Model ID\nmodel_id=\"google/flan-ul2\"", "execution_count": 21, "outputs": []}, {"metadata": {}, "id": "f606ec11", "cell_type": "code", "source": "# instantiate with access token and project ID from within Cloud project\nprompt = Prompt(access_token, project_id)", "execution_count": 22, "outputs": []}, {"metadata": {}, "id": "8053738c", "cell_type": "markdown", "source": "### 4.2 Send prompts to Watsonx.ai"}, {"metadata": {}, "id": "8e3ee91c", "cell_type": "code", "source": "# responses = [model.generate_text(prompt) for prompt in list_of_prompts]\nresponses = [prompt.generate(prompt_template_text, model_id, model_params)['generated_text'] for prompt_template_text in list_of_prompts]\n\nfor i, response in enumerate(responses):\n    lines = str(list_of_prompts[i]).strip().split(\"\\n\")\n    user_description = str(lines[8])  # TODO check hard-coded number\n    print(f\"\\n{user_description}\")\n    print(f\"\\n MODEL OUTPUT: {response}\")", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "\ninput: \"Michael Jones is 40 years old and lives in Seattle, WA. Their purchase history includes Toys games sporting_goods.\"\n\n MODEL OUTPUT: \"Recommendations:n Item 1: X-Box One - The X-Box One is the latest in the X-Box line of gaming systems. It features a high definition 1080p screen, a high-powered GPU, and a high-performance CPU. It also has Kinect, which allows you to play games by simply moving your body.nItem 2: X-Box 360 - The X\n\ninput: \"Ashley Brown is 20 years old and lives in Los Angeles, CA. Their purchase history includes Makeup skincare fashion.\"\n\n MODEL OUTPUT: \"Recommendations:n Item 1: Makeup - MAC Ruby Woo LipsticknItem 2: Skincare - Clinique Dramatically Different MoisturizernItem 3: Fashion - MAC x Patrick Starrr CollectionnItem 4: Location - Los Angeles, CAn\n\ninput: \"Emily Johnson is 55 years old and lives in Dallas, TX. Their purchase history includes Furniture appliances home_improvement_supplies.\"\n\n MODEL OUTPUT: \"Recommendations:n Item 1: RefrigeratornItem 2: FurniturenItem 3: AppliancesnItem 4: Home Improvement SuppliesnnItem 5: HomenItem 6: FurniturenItem 7: AppliancesnItem 8: HomenItem 9: Home Improvement SuppliesnItem 10: HomenItem 11: FurniturenItem 12: Home\n", "name": "stdout"}]}, {"metadata": {}, "id": "a1ab967e", "cell_type": "markdown", "source": "### Few shot prompt analysis\nThese results are not bad.  An X-Box for a customer with a history of buying toys and games.  Likewise cosmetics and furniture for the other two customers accurately reflects their purchase history.  "}, {"metadata": {}, "id": "d599c479", "cell_type": "markdown", "source": "## 5. Congratulations\nCongratulations on completing the lab and exploring the fascinating world of bulk creation of Few Shot Prompts using PromptTemplate! \n\nThrough the practical use case of generating personalized product recommendations, you have witnessed the power of tailoring prompts to individual customer profiles. By incorporating customer-specific details and programmatically generating bulk examples, you can fine-tune the model for your specific use case, resulting in more accurate and tailored outputs. \n\nContinuously iterating and refining your prompts based on these examples will unlock the full potential of language models and enhance their performance across various domains. Keep experimenting and leveraging prompt engineering techniques to optimize your interactions with language models and drive impactful results in your projects."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}